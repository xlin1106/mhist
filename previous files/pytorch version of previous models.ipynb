{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "264d8b90-596d-4a57-af05-f3f3756f6922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PYTORCH VERSION OF MNIST MODEL\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28594fe7-a86a-4dcc-95ef-faa7b7be3db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHISTDataset(Dataset):\n",
    "    def __init__(self, img_dir, labels_file, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.labels = pd.read_csv(labels_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.img_dir, self.labels.iloc[idx, 0])  # Image names from the first column\n",
    "        image = Image.open(img_name)\n",
    "\n",
    "        # Map 'hp' and 'ssa' to integer labels\n",
    "        label = 0 if self.labels.iloc[idx, 1] == 'HP' else 1  # Assuming 'HP' -> 0 and 'SSA' -> 1\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5f23b26-3693-4178-97df-257cbc1f3232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformations\n",
    "augmentation = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Path to images and the CSV file\n",
    "img_dir = './data/images'\n",
    "labels_file = './data/annotations.csv'\n",
    "\n",
    "# Load the CSV file and split based on the 'Partition' column\n",
    "annotations = pd.read_csv(labels_file)\n",
    "train_data = annotations[annotations['Partition'] == 'train']\n",
    "test_data = annotations[annotations['Partition'] == 'test']\n",
    "\n",
    "# Save the train and test splits into separate CSV files (if needed)\n",
    "train_data.to_csv('train_annotations.csv', index=False)\n",
    "test_data.to_csv('test_annotations.csv', index=False)\n",
    "\n",
    "# Create Dataset instances\n",
    "train_dataset = MHISTDataset(img_dir=img_dir, labels_file='train_annotations.csv', transform=augmentation)\n",
    "test_dataset = MHISTDataset(img_dir=img_dir, labels_file='test_annotations.csv', transform=transform)\n",
    "\n",
    "# Create DataLoader instances\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6e1c470-da54-49f9-8e85-c14a07c38721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model based off of MHIST\n",
    "class MHISTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MHISTModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 10, kernel_size=7, padding=0)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(10, 10, kernel_size=7, padding=0)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc1 = nn.Linear(10 * 11 * 11, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))  # Conv -> Relu -> Pooling\n",
    "        x = self.pool(torch.relu(self.conv2(x)))  # Conv -> Relu -> Pooling\n",
    "        x = x.view(x.size(0), -1)  # Flatten into 1-D\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.sigmoid(x)  # Output with sigmoid activation\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "660f533a-4b34-4255-9155-0d830ba6dad8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] - Training Loss: 0.6169 - Training Accuracy: 70.80% - Testing Loss: 0.6646 - Testing Accuracy: 63.15%\n",
      "Epoch [2/100] - Training Loss: 0.6073 - Training Accuracy: 71.03% - Testing Loss: 0.6720 - Testing Accuracy: 63.15%\n",
      "Epoch [3/100] - Training Loss: 0.6122 - Training Accuracy: 71.03% - Testing Loss: 0.6719 - Testing Accuracy: 63.15%\n",
      "Epoch [4/100] - Training Loss: 0.6103 - Training Accuracy: 71.03% - Testing Loss: 0.6624 - Testing Accuracy: 63.15%\n",
      "Epoch [5/100] - Training Loss: 0.6075 - Training Accuracy: 71.03% - Testing Loss: 0.6622 - Testing Accuracy: 63.15%\n",
      "Epoch [6/100] - Training Loss: 0.6068 - Training Accuracy: 71.03% - Testing Loss: 0.6635 - Testing Accuracy: 63.15%\n",
      "Epoch [7/100] - Training Loss: 0.6044 - Training Accuracy: 71.03% - Testing Loss: 0.6690 - Testing Accuracy: 63.15%\n",
      "Epoch [8/100] - Training Loss: 0.6071 - Training Accuracy: 71.03% - Testing Loss: 0.6687 - Testing Accuracy: 63.15%\n",
      "Epoch [9/100] - Training Loss: 0.6057 - Training Accuracy: 71.03% - Testing Loss: 0.6724 - Testing Accuracy: 63.15%\n",
      "Epoch [10/100] - Training Loss: 0.6039 - Training Accuracy: 71.03% - Testing Loss: 0.6640 - Testing Accuracy: 63.15%\n",
      "Epoch [11/100] - Training Loss: 0.6038 - Training Accuracy: 71.03% - Testing Loss: 0.6669 - Testing Accuracy: 63.15%\n",
      "Epoch [12/100] - Training Loss: 0.6072 - Training Accuracy: 71.03% - Testing Loss: 0.6690 - Testing Accuracy: 63.15%\n",
      "Epoch [13/100] - Training Loss: 0.6044 - Training Accuracy: 71.03% - Testing Loss: 0.6854 - Testing Accuracy: 63.15%\n",
      "Epoch [14/100] - Training Loss: 0.6031 - Training Accuracy: 71.03% - Testing Loss: 0.6748 - Testing Accuracy: 63.15%\n",
      "Epoch [15/100] - Training Loss: 0.6026 - Training Accuracy: 71.03% - Testing Loss: 0.6712 - Testing Accuracy: 63.15%\n",
      "Epoch [16/100] - Training Loss: 0.6028 - Training Accuracy: 71.03% - Testing Loss: 0.6673 - Testing Accuracy: 63.15%\n",
      "Epoch [17/100] - Training Loss: 0.6017 - Training Accuracy: 71.03% - Testing Loss: 0.6773 - Testing Accuracy: 63.15%\n",
      "Epoch [18/100] - Training Loss: 0.6034 - Training Accuracy: 71.03% - Testing Loss: 0.6653 - Testing Accuracy: 63.15%\n",
      "Epoch [19/100] - Training Loss: 0.6024 - Training Accuracy: 71.03% - Testing Loss: 0.6686 - Testing Accuracy: 63.15%\n",
      "Epoch [20/100] - Training Loss: 0.6020 - Training Accuracy: 71.03% - Testing Loss: 0.6696 - Testing Accuracy: 63.15%\n",
      "Epoch [21/100] - Training Loss: 0.6019 - Training Accuracy: 71.03% - Testing Loss: 0.6683 - Testing Accuracy: 63.15%\n",
      "Epoch [22/100] - Training Loss: 0.6023 - Training Accuracy: 71.03% - Testing Loss: 0.6700 - Testing Accuracy: 63.15%\n",
      "Epoch [23/100] - Training Loss: 0.6020 - Training Accuracy: 71.03% - Testing Loss: 0.6725 - Testing Accuracy: 63.15%\n",
      "Epoch [24/100] - Training Loss: 0.6008 - Training Accuracy: 71.03% - Testing Loss: 0.6672 - Testing Accuracy: 63.15%\n",
      "Epoch [25/100] - Training Loss: 0.6015 - Training Accuracy: 71.03% - Testing Loss: 0.6691 - Testing Accuracy: 63.15%\n",
      "Epoch [26/100] - Training Loss: 0.6010 - Training Accuracy: 71.03% - Testing Loss: 0.6615 - Testing Accuracy: 63.15%\n",
      "Epoch [27/100] - Training Loss: 0.6024 - Training Accuracy: 71.03% - Testing Loss: 0.6626 - Testing Accuracy: 63.15%\n",
      "Epoch [28/100] - Training Loss: 0.6001 - Training Accuracy: 71.03% - Testing Loss: 0.6612 - Testing Accuracy: 63.15%\n",
      "Epoch [29/100] - Training Loss: 0.5990 - Training Accuracy: 71.03% - Testing Loss: 0.6650 - Testing Accuracy: 63.15%\n",
      "Epoch [30/100] - Training Loss: 0.5981 - Training Accuracy: 71.03% - Testing Loss: 0.6624 - Testing Accuracy: 63.15%\n",
      "Epoch [31/100] - Training Loss: 0.5969 - Training Accuracy: 71.03% - Testing Loss: 0.6601 - Testing Accuracy: 63.15%\n",
      "Epoch [32/100] - Training Loss: 0.5967 - Training Accuracy: 71.03% - Testing Loss: 0.6557 - Testing Accuracy: 63.15%\n",
      "Epoch [33/100] - Training Loss: 0.5954 - Training Accuracy: 71.03% - Testing Loss: 0.6580 - Testing Accuracy: 63.15%\n",
      "Epoch [34/100] - Training Loss: 0.5958 - Training Accuracy: 71.03% - Testing Loss: 0.6575 - Testing Accuracy: 63.15%\n",
      "Epoch [35/100] - Training Loss: 0.5950 - Training Accuracy: 71.03% - Testing Loss: 0.6551 - Testing Accuracy: 63.15%\n",
      "Epoch [36/100] - Training Loss: 0.5975 - Training Accuracy: 71.03% - Testing Loss: 0.6577 - Testing Accuracy: 63.15%\n",
      "Epoch [37/100] - Training Loss: 0.5953 - Training Accuracy: 71.03% - Testing Loss: 0.6523 - Testing Accuracy: 63.15%\n",
      "Epoch [38/100] - Training Loss: 0.5956 - Training Accuracy: 71.03% - Testing Loss: 0.6549 - Testing Accuracy: 63.15%\n",
      "Epoch [39/100] - Training Loss: 0.5960 - Training Accuracy: 71.03% - Testing Loss: 0.6554 - Testing Accuracy: 63.15%\n",
      "Epoch [40/100] - Training Loss: 0.5933 - Training Accuracy: 71.03% - Testing Loss: 0.6564 - Testing Accuracy: 63.15%\n",
      "Epoch [41/100] - Training Loss: 0.5946 - Training Accuracy: 71.03% - Testing Loss: 0.6563 - Testing Accuracy: 63.15%\n",
      "Epoch [42/100] - Training Loss: 0.5947 - Training Accuracy: 71.03% - Testing Loss: 0.6535 - Testing Accuracy: 63.15%\n",
      "Epoch [43/100] - Training Loss: 0.5937 - Training Accuracy: 71.03% - Testing Loss: 0.6568 - Testing Accuracy: 63.15%\n",
      "Epoch [44/100] - Training Loss: 0.5926 - Training Accuracy: 71.03% - Testing Loss: 0.6606 - Testing Accuracy: 63.15%\n",
      "Epoch [45/100] - Training Loss: 0.5940 - Training Accuracy: 71.03% - Testing Loss: 0.6554 - Testing Accuracy: 63.15%\n",
      "Epoch [46/100] - Training Loss: 0.5944 - Training Accuracy: 71.03% - Testing Loss: 0.6599 - Testing Accuracy: 63.15%\n",
      "Epoch [47/100] - Training Loss: 0.5929 - Training Accuracy: 71.03% - Testing Loss: 0.6588 - Testing Accuracy: 63.15%\n",
      "Epoch [48/100] - Training Loss: 0.5937 - Training Accuracy: 71.03% - Testing Loss: 0.6545 - Testing Accuracy: 63.15%\n",
      "Epoch [49/100] - Training Loss: 0.5956 - Training Accuracy: 71.03% - Testing Loss: 0.6529 - Testing Accuracy: 63.15%\n",
      "Epoch [50/100] - Training Loss: 0.5937 - Training Accuracy: 71.03% - Testing Loss: 0.6554 - Testing Accuracy: 63.15%\n",
      "Epoch [51/100] - Training Loss: 0.5899 - Training Accuracy: 71.03% - Testing Loss: 0.6535 - Testing Accuracy: 63.15%\n",
      "Epoch [52/100] - Training Loss: 0.5929 - Training Accuracy: 71.03% - Testing Loss: 0.6556 - Testing Accuracy: 63.15%\n",
      "Epoch [53/100] - Training Loss: 0.5907 - Training Accuracy: 71.03% - Testing Loss: 0.6545 - Testing Accuracy: 63.15%\n",
      "Epoch [54/100] - Training Loss: 0.5918 - Training Accuracy: 71.03% - Testing Loss: 0.6546 - Testing Accuracy: 63.15%\n",
      "Epoch [55/100] - Training Loss: 0.5916 - Training Accuracy: 71.03% - Testing Loss: 0.6571 - Testing Accuracy: 63.15%\n",
      "Epoch [56/100] - Training Loss: 0.5936 - Training Accuracy: 71.03% - Testing Loss: 0.6577 - Testing Accuracy: 63.15%\n",
      "Epoch [57/100] - Training Loss: 0.5925 - Training Accuracy: 71.03% - Testing Loss: 0.6575 - Testing Accuracy: 63.15%\n",
      "Epoch [58/100] - Training Loss: 0.5919 - Training Accuracy: 71.03% - Testing Loss: 0.6520 - Testing Accuracy: 63.15%\n",
      "Epoch [59/100] - Training Loss: 0.5909 - Training Accuracy: 71.03% - Testing Loss: 0.6540 - Testing Accuracy: 63.15%\n",
      "Epoch [60/100] - Training Loss: 0.5933 - Training Accuracy: 71.03% - Testing Loss: 0.6555 - Testing Accuracy: 63.15%\n",
      "Epoch [61/100] - Training Loss: 0.5932 - Training Accuracy: 71.03% - Testing Loss: 0.6572 - Testing Accuracy: 63.15%\n",
      "Epoch [62/100] - Training Loss: 0.5909 - Training Accuracy: 71.03% - Testing Loss: 0.6542 - Testing Accuracy: 63.15%\n",
      "Epoch [63/100] - Training Loss: 0.5906 - Training Accuracy: 71.03% - Testing Loss: 0.6544 - Testing Accuracy: 63.15%\n",
      "Epoch [64/100] - Training Loss: 0.5924 - Training Accuracy: 71.03% - Testing Loss: 0.6541 - Testing Accuracy: 63.15%\n",
      "Epoch [65/100] - Training Loss: 0.5927 - Training Accuracy: 71.03% - Testing Loss: 0.6518 - Testing Accuracy: 63.15%\n",
      "Epoch [66/100] - Training Loss: 0.5924 - Training Accuracy: 71.03% - Testing Loss: 0.6550 - Testing Accuracy: 63.15%\n",
      "Epoch [67/100] - Training Loss: 0.5937 - Training Accuracy: 71.03% - Testing Loss: 0.6519 - Testing Accuracy: 63.15%\n",
      "Epoch [68/100] - Training Loss: 0.5929 - Training Accuracy: 71.03% - Testing Loss: 0.6527 - Testing Accuracy: 63.15%\n",
      "Epoch [69/100] - Training Loss: 0.5906 - Training Accuracy: 71.03% - Testing Loss: 0.6544 - Testing Accuracy: 63.15%\n",
      "Epoch [70/100] - Training Loss: 0.5916 - Training Accuracy: 71.03% - Testing Loss: 0.6533 - Testing Accuracy: 63.15%\n",
      "Epoch [71/100] - Training Loss: 0.5896 - Training Accuracy: 71.03% - Testing Loss: 0.6537 - Testing Accuracy: 63.15%\n",
      "Epoch [72/100] - Training Loss: 0.5904 - Training Accuracy: 71.03% - Testing Loss: 0.6522 - Testing Accuracy: 63.15%\n",
      "Epoch [73/100] - Training Loss: 0.5939 - Training Accuracy: 71.03% - Testing Loss: 0.6569 - Testing Accuracy: 63.15%\n",
      "Epoch [74/100] - Training Loss: 0.5914 - Training Accuracy: 71.03% - Testing Loss: 0.6561 - Testing Accuracy: 63.15%\n",
      "Epoch [75/100] - Training Loss: 0.5919 - Training Accuracy: 71.03% - Testing Loss: 0.6521 - Testing Accuracy: 63.15%\n",
      "Epoch [76/100] - Training Loss: 0.5935 - Training Accuracy: 71.03% - Testing Loss: 0.6559 - Testing Accuracy: 63.15%\n",
      "Epoch [77/100] - Training Loss: 0.5941 - Training Accuracy: 71.03% - Testing Loss: 0.6551 - Testing Accuracy: 63.15%\n",
      "Epoch [78/100] - Training Loss: 0.5926 - Training Accuracy: 71.03% - Testing Loss: 0.6544 - Testing Accuracy: 63.15%\n",
      "Epoch [79/100] - Training Loss: 0.5922 - Training Accuracy: 71.03% - Testing Loss: 0.6555 - Testing Accuracy: 63.15%\n",
      "Epoch [80/100] - Training Loss: 0.5925 - Training Accuracy: 71.03% - Testing Loss: 0.6557 - Testing Accuracy: 63.15%\n",
      "Epoch [81/100] - Training Loss: 0.5922 - Training Accuracy: 71.03% - Testing Loss: 0.6564 - Testing Accuracy: 63.15%\n",
      "Epoch [82/100] - Training Loss: 0.5900 - Training Accuracy: 71.03% - Testing Loss: 0.6555 - Testing Accuracy: 63.15%\n",
      "Epoch [83/100] - Training Loss: 0.5928 - Training Accuracy: 71.03% - Testing Loss: 0.6548 - Testing Accuracy: 63.15%\n",
      "Epoch [84/100] - Training Loss: 0.5930 - Training Accuracy: 71.03% - Testing Loss: 0.6568 - Testing Accuracy: 63.15%\n",
      "Epoch [85/100] - Training Loss: 0.5911 - Training Accuracy: 71.03% - Testing Loss: 0.6579 - Testing Accuracy: 63.15%\n",
      "Epoch [86/100] - Training Loss: 0.5926 - Training Accuracy: 71.03% - Testing Loss: 0.6507 - Testing Accuracy: 63.15%\n",
      "Epoch [87/100] - Training Loss: 0.5917 - Training Accuracy: 71.03% - Testing Loss: 0.6532 - Testing Accuracy: 63.15%\n",
      "Epoch [88/100] - Training Loss: 0.5913 - Training Accuracy: 71.03% - Testing Loss: 0.6558 - Testing Accuracy: 63.15%\n",
      "Epoch [89/100] - Training Loss: 0.5928 - Training Accuracy: 71.03% - Testing Loss: 0.6555 - Testing Accuracy: 63.15%\n",
      "Epoch [90/100] - Training Loss: 0.5923 - Training Accuracy: 71.03% - Testing Loss: 0.6518 - Testing Accuracy: 63.15%\n",
      "Epoch [91/100] - Training Loss: 0.5913 - Training Accuracy: 71.03% - Testing Loss: 0.6533 - Testing Accuracy: 63.15%\n",
      "Epoch [92/100] - Training Loss: 0.5922 - Training Accuracy: 71.03% - Testing Loss: 0.6557 - Testing Accuracy: 63.15%\n",
      "Epoch [93/100] - Training Loss: 0.5928 - Training Accuracy: 71.03% - Testing Loss: 0.6564 - Testing Accuracy: 63.15%\n",
      "Epoch [94/100] - Training Loss: 0.5928 - Training Accuracy: 71.03% - Testing Loss: 0.6533 - Testing Accuracy: 63.15%\n",
      "Epoch [95/100] - Training Loss: 0.5955 - Training Accuracy: 71.03% - Testing Loss: 0.6571 - Testing Accuracy: 63.15%\n",
      "Epoch [96/100] - Training Loss: 0.5920 - Training Accuracy: 71.03% - Testing Loss: 0.6560 - Testing Accuracy: 63.15%\n",
      "Epoch [97/100] - Training Loss: 0.5930 - Training Accuracy: 71.03% - Testing Loss: 0.6551 - Testing Accuracy: 63.15%\n",
      "Epoch [98/100] - Training Loss: 0.5937 - Training Accuracy: 71.03% - Testing Loss: 0.6524 - Testing Accuracy: 63.15%\n",
      "Epoch [99/100] - Training Loss: 0.5929 - Training Accuracy: 71.03% - Testing Loss: 0.6538 - Testing Accuracy: 63.15%\n",
      "Epoch [100/100] - Training Loss: 0.5907 - Training Accuracy: 71.03% - Testing Loss: 0.6540 - Testing Accuracy: 63.15%\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "\n",
    "# Model, loss function, and optimizer\n",
    "model = MHISTModel()\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.91)\n",
    "\n",
    "# Training and evaluation loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    # Training loop\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        labels = labels.float().unsqueeze(1)  # Reshape labels to (batch_size, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the training loss\n",
    "        running_train_loss += loss.item()\n",
    "\n",
    "        # Calculate training accuracy\n",
    "        predicted = (outputs > 0.5).float()  # Classify predictions\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "        total_train += labels.size(0)\n",
    "\n",
    "    # Average training loss and accuracy\n",
    "    avg_train_loss = running_train_loss / len(train_loader)\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "\n",
    "    # Testing loop\n",
    "    model.eval()\n",
    "    running_test_loss = 0.0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            labels = labels.float().unsqueeze(1)  # Reshape labels to (batch_size, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Accumulate the test loss\n",
    "            running_test_loss += loss.item()\n",
    "\n",
    "            # Calculate testing accuracy\n",
    "            predicted = (outputs > 0.5).float()  # Classify predictions\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "            total_test += labels.size(0)\n",
    "\n",
    "    # Average testing loss and accuracy\n",
    "    avg_test_loss = running_test_loss / len(test_loader)\n",
    "    test_accuracy = 100 * correct_test / total_test\n",
    "\n",
    "    # Adjust learning rate based on scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print epoch, training loss, training accuracy, testing loss, and testing accuracy\n",
    "    print(f'Epoch [{epoch + 1}/{epochs}] - '\n",
    "          f'Training Loss: {avg_train_loss:.4f} - Training Accuracy: {train_accuracy:.2f}% - '\n",
    "          f'Testing Loss: {avg_test_loss:.4f} - Testing Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d214692-c35a-4b2a-9100-09b6cab81e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "----------\n",
      "Train Loss: 0.6066 Acc: 0.6777\n",
      "Test Loss: 0.5991 Acc: 0.6520\n",
      "\n",
      "Epoch 2/100\n",
      "----------\n",
      "Train Loss: 0.5298 Acc: 0.7301\n",
      "Test Loss: 0.5591 Acc: 0.7001\n",
      "\n",
      "Epoch 3/100\n",
      "----------\n",
      "Train Loss: 0.4783 Acc: 0.7490\n",
      "Test Loss: 0.5190 Acc: 0.7083\n",
      "\n",
      "Epoch 4/100\n",
      "----------\n",
      "Train Loss: 0.4570 Acc: 0.7747\n",
      "Test Loss: 0.5113 Acc: 0.7329\n",
      "\n",
      "Epoch 5/100\n",
      "----------\n",
      "Train Loss: 0.4404 Acc: 0.7954\n",
      "Test Loss: 0.4925 Acc: 0.7503\n",
      "\n",
      "Epoch 6/100\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 87\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 50\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(phase \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 50\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     _, preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     52\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/models/resnet.py:276\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[1;32m    275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n\u001b[0;32m--> 276\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[1;32m    279\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/models/resnet.py:147\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    144\u001b[0m identity \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    146\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\n\u001b[0;32m--> 147\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[1;32m    150\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(out)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:176\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    169\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:2512\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2510\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2512\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2513\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2514\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "import time\n",
    "import copy\n",
    "\n",
    "# Use pre-trained ResNet-50 and modify the final layer for the number of classes\n",
    "num_classes = 2\n",
    "model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "# Define loss function, optimizer, and learning rate scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.91)\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=100):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'test']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "                dataloader = train_loader\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "                dataloader = test_loader\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data\n",
    "            for inputs, labels in dataloader:\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Backward pass + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloader.dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
    "\n",
    "            print(f'{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # Deep copy the model if it has the best accuracy\n",
    "            if phase == 'test' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        # Step the learning rate scheduler after each epoch\n",
    "        scheduler.step()\n",
    "\n",
    "        print()\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "trained_model = train_model(model, criterion, optimizer, scheduler, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5435485e-b9e7-4099-97eb-748cfab03fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
